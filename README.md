# imbalanced_missingValues_handling
open the collab to see all the results , we removed the missing values with different techs like dropping the rows that contain them or do the mean or knn and we compare it them then we choose one of them to keep working we got an imbalanced dataset where there's classes that are more represented than the others that's why the model can predict the well represented and the others recall is 0 when we did oversampling with smote or oversampling with under when we use it smoteenn we got a more balanced training dataset that we will train the model based on her and indeed we got better results for all the classes but the class 9 due to have much more less than the others 5 occurences only that means only one occurence in testing so the recall is still 0 also the best one class 6 used to 80% recall and it will reduce a lot at the cost of having better recalls for the other classes , we can suggest cross-validation as a solution here 
